Multi_Features Linear REGRESSION
================================
In Assignment 1 we did uni-feature linear regression. In this assignment, we will do multi-feature linear regression. And, we will use vectorized implementation using numpy so that we have a very compact code.


Information that you might need to complete Assignment:

 - We have provided CSVutils.py to load data. Data can be loaded by following command:

 	X,y = csv.readCSV('03_multiFeatureLinearReg.csv')
 	
    > where X is an [m x (n+1)] numpy matrix of features, 
    > x0, x1, x2, x3 are features where x0 is 1.
    > and y is the target value which we want to predict.

 - hypothesis Equation h(x) = t0*x0 + t1*x1 + t2*x2 + t3*x3
   This is to be implemented as a dot product of X and theta, i.e.

   h = x.dot(theta);     # see linearRegressionModel.py
   h is an (m x 1) vector.
	
   transpose(thetas)*X = [t0 t1 t2 t3] * [x00 x01 x02 x03 ...
                                          x10 x11 x12 x13 ...
                                          x20 x21 x22 x23 ...
                                          x30 x31 x42 x43 ...]

    note : transpose(theta) * X = transpose(X) * theta

   now we want to optimal t0,t1,t2,t3 (use Gradient decent to find Optimal Thetas)
 
 - The loss function can be implemented as:

 	h  = hypothesis(theta, X);
	error = 0.5/m * np.sum((h - y)**2)     # again see see linearRegressionModel.py

 - you can develop a general equation to updata all thetas instead of writing separate equations
   For example the a compact implementation will look like:

   		dJdx =  np.sum(....);                # figure it out yourself
   	    theta = theta - (alpha / m) * dJdx   # this is a compact implementation of update law

 - you can use "linearRegressionModel.py" code to compute hypothesis and cost Function

